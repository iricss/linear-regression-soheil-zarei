{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Lets assume that we have $N$ observation and $M$ features. The problem of linear regression is defined as:\n",
    "\n",
    "$$\\vec{y}_p = \\boldsymbol{X} \\vec{w},$$\n",
    "* $\\vec{y}_p$, a vector of size $N$, represents our predictions. \n",
    "* $\\boldsymbol{X}$ is a matrix of $(N\\times M)$. \n",
    "* $\\vec{w}$ is the fitting parameters, its our job to find them.\n",
    "\n",
    "Let measure how good are predictions are, we use Mean Squared Error (MSE) to calculate the distance of our predictions from true values:\n",
    "\n",
    "$$J = (\\vec{y}_p - \\vec{y}_t)^2$$\n",
    "\n",
    "* $\\vec{y}_t$ is a vector of true values. \n",
    "* $J$ is MSE and it is an scalar.\n",
    "\n",
    "Now our job is to find $\\vec{w}$ in such a way that it minimizes the cost function. Here we are going to use Stochastic Gradient Descent (SGD) to do that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_t, y_p):\n",
    "    '''returns the mean squared error.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_t : numpy array, shape (n_samples,) \n",
    "        True labels.\n",
    "    y_p : numpy array (float), shape (n_samples,)\n",
    "        Output of classifier (not lables).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Returns MSE.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> import numpy as np\n",
    "    >>> t = np.array([-1,1,1])\n",
    "    >>> y = np.array([1,-1,0])\n",
    "    >>> mse(t, y)\n",
    "    3.0\n",
    "    '''\n",
    "    d = y_t-y_p\n",
    "    return np.dot(d,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_mse(X, y_t, y_p):\n",
    "    '''\n",
    "    Gradient of mean squared error. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (n_smaples, n_features)\n",
    "        Matrix of features.    \n",
    "    y_t : numpy array, shape (n_samples,)\n",
    "        True labels.\n",
    "    y_p : numpy array (float), shape (n_samples,)\n",
    "        Output of classifier (not lables).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape (n_features,)\n",
    "        Returns the gradient of MSE.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[-1,-1],[1,1],[0,1]])\n",
    "    >>> y_t = np.array([-1,1,1])\n",
    "    >>> y_p = np.array([1,0,1])\n",
    "    >>> d_mse(X, y_t, y_p)\n",
    "    array([-2., -2.])\n",
    "    '''\n",
    "    y = y_p - y_t\n",
    "    n = np.size(y_t)\n",
    "    return (2/n)*(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "return 1.0 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(epoches=1000,alpha=1.0,eta=0.0001,normalize=True,tol=0.00001):\n",
    "    '''\n",
    "    Gradient Descent learning. \n",
    "    \n",
    "    The default loss function is MSE and the default penalty is l2. \n",
    "    \n",
    "    If the test set is provided, it keeps running until the cost stop decreasing. \n",
    "    If the test set is not provided, it keeps running until the improvment in the cost is less than **tol**.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta: float \n",
    "        Learning rate, default to 0.0001.\n",
    "        \n",
    "    alpha: float\n",
    "        Regularization parameter, defaults to 1.0\n",
    "\n",
    "    \n",
    "    epoches: int\n",
    "        number of epoches to train, defaults to 1000.\n",
    "    \n",
    "    normalize: bool\n",
    "        True: normalize data by mean and std (default).\n",
    "        False: do not change the data.\n",
    "        \n",
    "    tol: float \n",
    "        Stop training if the improvment in cost function is less than tol (when test set is not provided). \n",
    "        Defaults to 0.00001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape (n_features,)\n",
    "        Returns the weights (fitting parameters).\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> #import numpy as np\n",
    "    >>> #import matplotlib.pyplot as plt\n",
    "    ... (write your test heere.)\n",
    "    '''\n",
    "    ''''\n",
    "    X = np.array([[-1,-1],[1,1],[0,1]])\n",
    "    y_t = np.array([-1,1,1])\n",
    "    y_p = np.array([1,0,1])\n",
    "\n",
    "    w[1] = np.array([.25,.2,.1])\n",
    "    z = 20\n",
    "\n",
    "    for i in rang(10):\n",
    "        tmp = \n",
    "        w[i] = w[i-1] - z.dot(d_mse(X, y_t, y_p))\n",
    "  \n",
    "    w_y = np.array([.25,.2,.1])\n",
    "    for i in rang(10):\n",
    "        tmp = w_y - z.dot(d_mse(X, y_t, y_p))\n",
    "    print(w[10])\n",
    "    ''''\n",
    "    (X, y_t) = make_blobs(n_samples=250, n_features=2, centers=2,cluster_std=1.05, random_state=20)\n",
    "    X = np.c_[np.ones((X.shape[0])), X]\n",
    "\n",
    "    W = np.random.uniform(size=(X.shape[1],))\n",
    "    \n",
    "    lossHistory = []\n",
    "    \n",
    "    for epoch in np.arange(0, epoches):\n",
    "        y_p = sigmoid_activation(X.dot(W))\n",
    "        loss = mse(y_p,y_t)\n",
    "        if loss > tol\n",
    "            lossHistory.append(loss)\n",
    "            print(\"[INFO] epoch #{}, loss={:.7f}\".format(epoch + 1, loss))\n",
    "            gradient = X.T.dot(error) / X.shape[0]\n",
    "            W += -eta * gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 38, in __main__.GD\n",
      "Failed example:\n",
      "    #import matplotlib.pyplot as plt\n",
      "    (write your test heere.)\n",
      "Exception raised:\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\doctest.py\", line 1330, in __run\n",
      "        compileflags, 1), test.globs)\n",
      "      File \"<doctest __main__.GD[0]>\", line 2\n",
      "        (write your test heere.)\n",
      "                  ^\n",
      "    SyntaxError: invalid syntax\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   1 of   1 in __main__.GD\n",
      "***Test Failed*** 1 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(w):\n",
    "    '''\n",
    "    Return l2 penalty. \n",
    "    w : weights of the model, a numpy vector of (n_features)\n",
    "    '''\n",
    "\n",
    "def d_l2(w):\n",
    "    '''\n",
    "    Return gradient of l2 penalty.\n",
    "    w : weights of the model, a numpy vector of (n_features)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(X, y_t, y_p, eta=0.0001, alpha=1.0, b = 100, epoches=1000, tol=0.00001):\n",
    "    '''\n",
    "    mini-batch Stochastic Gradient Descent learning. \n",
    "    \n",
    "    The default loss function is MSE and the default penalty is l2. \n",
    "    \n",
    "    If the test set is provided, it keeps running until the cost stop decreasing. \n",
    "    If the test set is not provided, it keeps running until the improvment in the cost is less than **tol**.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta: float \n",
    "        Learning rate, default to 0.0001.\n",
    "        \n",
    "    alpha: float\n",
    "        Regularization parameter, defaults to 1.0\n",
    "        \n",
    "    b: int \n",
    "        batch size, defaults to 100.\n",
    "    \n",
    "    epoches: int\n",
    "        number of epoches to train, defaults to 1000.\n",
    "    \n",
    "    normalize: bool\n",
    "        True: normalize data by mean and std (default).\n",
    "        False: do not change the data.\n",
    "        \n",
    "    tol: float \n",
    "        Stop training if the improvment in cost function is less than tol (when test set is not provided). \n",
    "        Defaults to 0.00001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape (n_features,)\n",
    "        Returns the weights (fitting parameters).\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> import numpy as np\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    ... (write your test heere.)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a simple linear model $y =  x + 1$. Given x, this gives us the true value of $y$, ($y_t$). \n",
    "Generate some $x$ and $y$ pairs, use your code to infer fitting parameters. Plot the ground truth $y = x +1$ and your fitted model for the interval of $x \\in [-2,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate some other $x$ and $y$ pairs, add a normally distributed noise ($\\mu =0 $ $\\sigma = 0.1$) to $y$ and infer fitting parameters again. Plot the theory and fitted line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
